# hyperparameter_tuning.py
import pandas as pd
import numpy as np
from sklearn.model_selection import GridSearchCV, RandomizedSearchCV, cross_val_score
from sklearn.metrics import make_scorer, roc_auc_score
from xgboost import XGBClassifier
from scipy.stats import uniform, randint
import joblib
import os
import time

class HyperparameterTuner:
    def __init__(self, X, y):
        self.X = X
        self.y = y
        self.best_params = None
        self.best_score = 0
        self.tuning_results = {}
        
        # è®¡ç®—æ­£è´Ÿæ ·æœ¬æ¯”ä¾‹ï¼ˆç”¨äºscale_pos_weightï¼‰
        neg_count = np.sum(y == 0)
        pos_count = np.sum(y == 1)
        self.scale_pos_weight = neg_count / pos_count
        
    def grid_search_tuning(self):
        """ç½‘æ ¼æœç´¢è°ƒä¼˜"""
        print("å¼€å§‹ç½‘æ ¼æœç´¢è°ƒä¼˜...")
        
        # å‚æ•°ç½‘æ ¼
        param_grid = {
            'n_estimators': [100, 200, 300],
            'max_depth': [3, 5, 7],
            'learning_rate': [0.01, 0.1, 0.2],
            'subsample': [0.8, 1.0],
            'colsample_bytree': [0.8, 1.0],
            'scale_pos_weight': [self.scale_pos_weight]
        }
        
        # åˆ›å»ºæ¨¡å‹
        xgb = XGBClassifier(random_state=42, eval_metric='logloss')
        
        # ç½‘æ ¼æœç´¢
        grid_search = GridSearchCV(
            estimator=xgb,
            param_grid=param_grid,
            scoring='roc_auc',
            cv=3,  # å‡å°‘CVæŠ˜æ•°ä»¥èŠ‚çœæ—¶é—´
            n_jobs=-1,
            verbose=1
        )
        
        start_time = time.time()
        grid_search.fit(self.X, self.y)
        end_time = time.time()
        
        # ä¿å­˜ç»“æœ
        self.tuning_results['grid_search'] = {
            'best_params': grid_search.best_params_,
            'best_score': grid_search.best_score_,
            'time_taken': end_time - start_time,
            'cv_results': grid_search.cv_results_
        }
        
        print(f"âœ… ç½‘æ ¼æœç´¢å®Œæˆï¼æœ€ä½³åˆ†æ•°: {grid_search.best_score_:.4f}")
        print(f"æœ€ä½³å‚æ•°: {grid_search.best_params_}")
        print(f"è€—æ—¶: {end_time - start_time:.2f} ç§’")
        
        return grid_search.best_estimator_
    
    def bayesian_optimization_tuning(self):
        """è´å¶æ–¯ä¼˜åŒ–è°ƒä¼˜ï¼ˆä½¿ç”¨Optunaï¼‰"""
        try:
            import optuna
            
            print("å¼€å§‹è´å¶æ–¯ä¼˜åŒ–è°ƒä¼˜...")
            
            def objective(trial):
                # å®šä¹‰å‚æ•°ç©ºé—´
                params = {
                    'n_estimators': trial.suggest_int('n_estimators', 100, 1000),
                    'max_depth': trial.suggest_int('max_depth', 3, 10),
                    'learning_rate': trial.suggest_float('learning_rate', 0.01, 0.3),
                    'subsample': trial.suggest_float('subsample', 0.6, 1.0),
                    'colsample_bytree': trial.suggest_float('colsample_bytree', 0.6, 1.0),
                    'scale_pos_weight': self.scale_pos_weight,
                    'random_state': 42,
                    'eval_metric': 'logloss'
                }
                
                # åˆ›å»ºæ¨¡å‹
                model = XGBClassifier(**params)
                
                # äº¤å‰éªŒè¯è¯„åˆ†
                cv_scores = cross_val_score(model, self.X, self.y, cv=3, scoring='roc_auc')
                return cv_scores.mean()
            
            # åˆ›å»ºç ”ç©¶å¯¹è±¡
            study = optuna.create_study(direction='maximize')
            
            start_time = time.time()
            study.optimize(objective, n_trials=50, show_progress_bar=True)
            end_time = time.time()
            
            # ä¿å­˜ç»“æœ
            self.tuning_results['bayesian_optimization'] = {
                'best_params': study.best_params,
                'best_score': study.best_value,
                'time_taken': end_time - start_time,
                'study': study
            }
            
            print(f"âœ… è´å¶æ–¯ä¼˜åŒ–å®Œæˆï¼æœ€ä½³åˆ†æ•°: {study.best_value:.4f}")
            print(f"æœ€ä½³å‚æ•°: {study.best_params}")
            print(f"è€—æ—¶: {end_time - start_time:.2f} ç§’")
            
            # è®­ç»ƒæœ€ä½³æ¨¡å‹
            best_params = study.best_params.copy()
            best_params.update({
                'scale_pos_weight': self.scale_pos_weight,
                'random_state': 42,
                'eval_metric': 'logloss'
            })
            best_model = XGBClassifier(**best_params)
            best_model.fit(self.X, self.y)
            
            return best_model
            
        except ImportError:
            print("âš ï¸ Optunaæœªå®‰è£…ï¼Œè·³è¿‡è´å¶æ–¯ä¼˜åŒ–")
            print("å®‰è£…å‘½ä»¤: pip install optuna")
            return None
    
    def compare_tuning_methods(self):
        """æ¯”è¾ƒä¸åŒè°ƒä¼˜æ–¹æ³•çš„ç»“æœ"""
        print("\n=== è¶…å‚æ•°è°ƒä¼˜æ–¹æ³•æ¯”è¾ƒ ===")
        
        methods = []
        scores = []
        times = []
        
        if 'grid_search' in self.tuning_results:
            methods.append('Grid Search')
            scores.append(self.tuning_results['grid_search']['best_score'])
            times.append(self.tuning_results['grid_search']['time_taken'])
            
        if 'bayesian_optimization' in self.tuning_results:
            methods.append('Bayesian Optimization')
            scores.append(self.tuning_results['bayesian_optimization']['best_score'])
            times.append(self.tuning_results['bayesian_optimization']['time_taken'])
        
        # åˆ›å»ºæ¯”è¾ƒDataFrame
        comparison_df = pd.DataFrame({
            'Method': methods,
            'Best Score': scores,
            'Time (seconds)': times
        })
        
        print(comparison_df.to_string(index=False))
        
        # é€‰æ‹©æœ€ä½³æ–¹æ³•
        best_method_idx = np.argmax(scores)
        best_method = methods[best_method_idx]
        self.best_params = (self.tuning_results['grid_search']['best_params'] 
                          if best_method == 'Grid Search' 
                          else self.tuning_results['bayesian_optimization']['best_params'])
        self.best_score = scores[best_method_idx]
        
        print(f"\nğŸ† æœ€ä½³è°ƒä¼˜æ–¹æ³•: {best_method}")
        print(f"æœ€ä½³å‚æ•°: {self.best_params}")
        
        return comparison_df
    
    def save_tuning_results(self):
        """ä¿å­˜è°ƒä¼˜ç»“æœ"""
        os.makedirs('results/tuning', exist_ok=True)
        
        # ä¿å­˜æœ€ä½³å‚æ•°
        with open('results/tuning/best_params.txt', 'w') as f:
            f.write(str(self.best_params))
        
        # ä¿å­˜è¯¦ç»†ç»“æœ
        joblib.dump(self.tuning_results, 'results/tuning/tuning_results.pkl')
        
        print("âœ… è°ƒä¼˜ç»“æœå·²ä¿å­˜")

# ä½¿ç”¨ç¤ºä¾‹
if __name__ == "__main__":
    # å®‰è£…optunaï¼ˆå¦‚æœéœ€è¦ï¼‰
    # pip install optuna
    
    # åŠ è½½æ•°æ®
    X = np.load('processed_data/X_train.npy')
    y = np.load('processed_data/y_train.npy')
    
    # åˆ›å»ºè°ƒä¼˜å™¨
    tuner = HyperparameterTuner(X, y)
    
    # æ‰§è¡Œç½‘æ ¼æœç´¢
    grid_model = tuner.grid_search_tuning()
    
    # æ‰§è¡Œè´å¶æ–¯ä¼˜åŒ–ï¼ˆå¯é€‰ï¼‰
    bayesian_model = tuner.bayesian_optimization_tuning()
    
    # æ¯”è¾ƒç»“æœ
    comparison_df = tuner.compare_tuning_methods()
    
    # ä¿å­˜ç»“æœ
    tuner.save_tuning_results()
    
    # ä¿å­˜æœ€ä½³æ¨¡å‹ï¼ˆåŸºäºç½‘æ ¼æœç´¢ç»“æœï¼‰
    if grid_model is not None:
        joblib.dump(grid_model, 'models/tuned_xgboost_model.pkl')
        print("âœ… è°ƒä¼˜åçš„æ¨¡å‹å·²ä¿å­˜: models/tuned_xgboost_model.pkl")